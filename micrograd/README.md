# Micrograd (Work in Progress)

<p style="font-size: 5 px;"><i>This repository contains an educational implementation of a micro-scale automatic differentiation engine, inspired by micrograd
 by Andrej Karpathy.
It is intended for learning and experimenting with the fundamentals of backpropagation and neural networks from scratch â€” without relying on deep learning frameworks like PyTorch or TensorFlow.
<br>
<H4> Project Goals </H4>
<p style="font-size: 5px;"><i><ul>
<li>Build a simple scalar-based autograd engine </li>
<li>Implement backpropagation step-by-step</li>
<li>Create and train a minimal neural network using only Python + NumPy</li>
<li>Learn by coding every part of the system</li>
</ul></i><p>
<H4>Requirements</H4>
<p style="font-size: 5px;"><i> 
<ul>
  <li>Python 3.8+</li>
  <li>NumPy</li>
  <li>Jupyter Notebook</li>
</ul>

<H4>Contributing</H4>
This project is at a very early stage. Contributions, issues, and suggestions are welcome as the implementation grows.
</i></p>
