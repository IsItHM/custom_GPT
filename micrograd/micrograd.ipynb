{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fe046e",
   "metadata": {},
   "source": [
    "# Building Micrograd\n",
    "###### I will build micrograd,an autograd engine (automatic differentiation engine), which is the heart of PyTorch, TensorFlow, etc. and then solve the exercises here given by andrej karpathy in his youtube playlist hero to zero playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ae812",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12px;\"><i><u>Autograd</u> - Automatic gradient. Which implements backpropagation. Micrograd is built to understand maths of neural network at the fundumental levels. So we will not use our inputs as tensors or vectors here. In modern Deep neural network tensors or vectors are used for efficiency and to make the algorithms run faster. which we will not wory for now.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0d6f2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12px;\"> <i> The %matplotlib inline command is a \"magic command\" in Jupyter Notebook. Its purpose is to ensure that Matplotlib plots are displayed directly below the code cell that generates them, rather than in a separate pop-up window. This creates an integrated and cohesive workflow where your code, data, and visualizations are all in one place within the notebook.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x +5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7261c5",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12px;\"> <i>just a random function which takes an scaler input and give an scaler output.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912325ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce79ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs= np.arange(-5, 5, 0.25 )\n",
    "ys=f(xs)\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f6801",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12px;\"> <i> np.arrange(start,stop,step) -> np.arange(-5,5,0.25):which means This creates an array [-5,-4.5,...,4.5], starting at -5, ending before 5, with a step of 0.5. Using floating-point numbers.</i><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7318ba2",
   "metadata": {},
   "source": [
    " $\\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} $\n",
    "<p style=\"font-size: 12px;\"> <i>This is slope/derivative of our equation </i></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.000001\n",
    "x=2/3 #at x=2/3 the slope becomes almost 0.\n",
    "(f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94598b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get a bit complex\n",
    "a=2.0\n",
    "b=-3.0\n",
    "c=10.0\n",
    "d=a*b+c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ba95d",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12px;\"> <i> Now, we will figure out the derivative of d with repect to a,b and c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622eff40",
   "metadata": {},
   "source": [
    "$ \\frac{d}{da}(d) = b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7accf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.000001\n",
    "a=2.0\n",
    "b=-3.0\n",
    "c=10.0\n",
    "d1=a*b+c\n",
    "a+=h\n",
    "d2=a*b+c\n",
    "slope=(d2-d1)/h\n",
    "print(\"derivation of d with respect to a: \",slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e540370",
   "metadata": {},
   "source": [
    "$ \\frac{d}{db}(d) = a $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.000001\n",
    "a=2.0\n",
    "b=-3.0\n",
    "c=10.0\n",
    "d1=a*b+c\n",
    "b+=h\n",
    "d2=a*b+c\n",
    "slope=(d2-d1)/h\n",
    "print(\"derivation of d with respect to b: \",slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c3b2d",
   "metadata": {},
   "source": [
    "$ \\frac{d}{dc}(d)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=0.000001\n",
    "a=2.0\n",
    "b=-3.0\n",
    "c=10.0\n",
    "d1=a*b+c\n",
    "c+=h\n",
    "d2=a*b+c\n",
    "slope=(d2-d1)/h\n",
    "print(\"derivation of d with recpect to c :\",slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9549d6",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12px;\"> <i>Right now, it looks like we’re just reinventing floats. But the big picture is:<br>\n",
    "A neural network does tons of multiplications and additions.\n",
    "Example: y = w*x + b is the formula for a neuron.<br>\n",
    "Later, we’ll need to do backpropagation → computing gradients (dy/dw, dy/db).\n",
    "Python’s built-in float can’t keep track of these operations, but our custom Value can.<br>\n",
    " This Value class is the starting point for:\n",
    "Storing a number (data).<br>\n",
    "Remembering how it was created (from addition/multiplication).\n",
    "So, this is step 1 of building an autograd engine</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affbc0b",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12px;\"><i>\n",
    "self.data→ stores the actual number.<br>\n",
    "self._prev:<br>\n",
    "Keeps track of which Value objects were used to create this one.<br>\n",
    "Example: if d = a * b, then d._prev = {a, b}.<br>\n",
    "This is the “computation graph” idea → each value knows its parents.<br>\n",
    "self._op:\n",
    "Stores the operation that created this value (+ or *).\n",
    "Helps us know what happened during forward pass, and later helps with backprop.\n",
    "</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c732757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self,data,_children=(),_op=''):\n",
    "        self.data=data\n",
    "        self._prev=set(_children)\n",
    "        self._op=_op\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "#this repr(re presenter) function is used to get output in our desired format\n",
    "# the code will run even without this function but \n",
    "# we wont understand the output    \n",
    "    def __add__(self,other):\n",
    "        out=Value(self.data+other.data,(self,other),'+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        out=Value(self.data*other.data,(self,other),'*')\n",
    "        return out\n",
    "a=Value(2.0)\n",
    "b=Value(-3.0)\n",
    "c=Value(10.0)\n",
    "d=a*b+c\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403e92f",
   "metadata": {},
   "source": [
    "Now we will visualize, so we will need graphviz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
